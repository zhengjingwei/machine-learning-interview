[TOC]

# 一、机器学习相关

## 1、 基本概念

- [ ] [1-1  简述解决一个机器学习问题时，你的流程是怎样的？](#1-1)

- [x] [1-2  损失函数是什么，如何定义合理的损失函数？](#1-2)

- [ ] [1-3  回归模型和分类模型常用损失函数有哪些？各有什么优缺点](#1-3)

- [ ] [1-4  什么是结构误差和经验误差？训练模型的时候如何判断已经达到最优？](#1-4)

- [ ] [1-5  模型的“泛化”能力是指？如何提升模型泛化能力？](#1-5)

- [x] [1-6  如何选择合适的模型评估指标？AUC、精准度、召回率、F1值都是什么？如何计算？有什么优缺点？](#1-6)

- [x] [1-7  什么是混淆矩阵？](#1-7)

- [x] [1-8  ROC曲线如何绘制？相比P-R曲线有什么特点？](#1-8)

- [x] [1-9  如何评判模型是过拟合还是欠拟合？遇到过拟合或欠拟合时，你是如何解决？](#1-9)

- [ ] [1-10  你是如何针对应用场景选择合适的模型？](#1-10)

- [x] [1-11  如何选择模型中的超参数？有什么方法，并说说其优劣点](#1-11)

- [ ] [1-12  误差分析是什么？你是如何进行误差分析？](#1-12)

- [ ] [1-13  你是如何理解模型的偏差和方差？什么样的情况是高偏差，什么情况是高方差？](#1-13)

- [ ] [1-14  出现高偏差或者高方差的时候你有什么优化策略？](#1-14)

- [ ] [1-15  奥卡姆剃刀定律是什么？对机器学习模型优化有何启发？举例说明](#1-15)

- [ ] [1-16  线性模型和非线性模型的区别？哪些模型是线性模型，哪些模型是非线性模型？](#1-16)

- [ ] [1-17  生成式模型和判别式模型的区别？哪些模型是生成式模型，哪些模型是判别式模型？](#1-17)



## 2、经典机器学习

### **特征工程**

- [ ] [2-1-1  你是怎样理解“特征”？](#2-1-1)

- [ ] [2-1-2  给定场景和问题，你如何设计特征？（特征工程方法论）](#2-1-2)

- [ ] [2-1-3  开发特征时候做如何做数据探索，怎样选择有用的特征？](#2-1-3)

- [ ] [2-1-4  你是如何做数据清洗的？举例说明](#2-1-4)

- [ ] [2-1-5  如何发现数据中的异常值，你是如何处理？](#2-1-5)

- [ ] [2-1-6  缺失值如何处理？](#2-1-6)

- [ ] [2-1-7  对于数值类型数据，你会怎样处理？为什么要做归一化？归一化有哪些方法？离散些方法，离散化和归一化有哪些优缺点？](#2-1-7)

- [ ] [2-1-8  标准化和归一化异同？](#2-1-8)

- [ ] [2-1-9  你是如何处理CTR类特征？](#2-1-9)

- [ ] [2-1-10  讲解贝叶斯平滑原理？以及如何训练得到平滑参数](#2-1-10)

- [ ] [2-1-11  类别型数据你是如何处理的？比如游戏品类，地域，设备](#2-1-11)

- [ ] [2-1-12  序号编码、one-hot编码、二进制编码都是什么？适合怎样的类别型数据？](#2-1-12)

- [ ] [2-1-13  时间类型数据你的处理方法是什么？原因？](#2-1-13)

- [ ] [2-1-14  你怎样理解组合特征？举个例子，并说明它和单特征有啥区别](#2-1-14)

- [ ] [2-1-15  如何处理高维组合特征？比如用户ID和内容ID？](#2-1-15)

- [ ] [2-1-16  如何理解笛卡尔积、外积、内积？](#2-1-16)

- [ ] [2-1-17  文本数据你会如何处理？](#2-1-17)

- [ ] [2-1-18  文本特征表示有哪些模型？他们的优缺点都是什么？](#2-1-18)

- [ ] [2-1-19  讲解TFF原理，它有什么优点和缺点？针对它的缺点，你有什么优化思路？](#2-1-19)

- [ ] [2-1-20  N-gram算法是什么？有什么优缺点？](#2-1-20)

- [ ] [2-1-21  讲解一下word2vec工作原理？损失函数是什么？](#2-1-21)

- [ ] [2-1-22  讲解一下LDA模型原理和训练过程？](#2-1-22)

- [ ] [2-1-23  Word2vec和LDA两个模型有什么区别和联系？](#2-1-23)

- [ ] [2-1-24  Skin-gram和cbow有何异同？](#2-1-24)

- [ ] [2-1-25  图像数据如何处理？有哪些常用的图像特征提取方法](#2-1-25)

- [ ] [2-1-26  你是怎样做特征选择的？卡方检验、信息值（IV）、VOE都是如何计算？](#2-1-26)

- [ ] [2-1-27  计算特征之间的相关性方法有哪些？有什么优缺点](#2-1-27)



**基础算法原理和推导**

**KNN**

- [ ] [2-2-1  Knn建模流程是怎样的？](#2-2-1)

- [ ] [2-2-2  Knn优缺点是什么？](#2-2-2)

- [ ] [2-2-3  Knn适合什么样的场景和数据类型？](#2-2-3)

- [ ] [2-2-4  常用的距离衡量公式都有哪些？具体说明它们的计算流程，以及使用场景？](#2-2-4)

- [ ] [2-2-5  超参数K值过大或者过小对结果有什么影响，你是如何选择K值？](#2-2-5)

- [ ] [2-2-6  介绍一下Kd树？如何建树，以及如何搜索最近节点？](#2-2-6)

  ·

**支持向量机**

- [ ] [2-3-1  简单讲解SVM模型原理？](#2-3-1)

- [ ] [2-3-2  SVM为什么会对缺失值敏感？实际应用时候你是如何处理？](#2-3-2)

- [ ] [2-3-3  SVM为什么可以分类非线性问题？](#2-3-3)

- [ ] [2-3-4  常用的核函数有哪些？你是如何选择不同的核函数的？](#2-3-4)

- [ ] [2-3-5  RBF核函数一定线性可分么？为什么](#2-3-5)

- [ ] [2-3-6  SVM属于线性模型还是非线性模型？为什么？](#2-3-6)

- [ ] [2-3-7  训练误差为0的SVM分类器一定存在吗？说明原因？](#2-3-7)



**朴素贝叶斯模型**

- [ ] [2-4-1  讲解贝叶斯定理？](2-4-1)

- [ ] [2-4-2  什么是条件概率、边缘概率、联合概率？](#2-4-2)

- [ ] [2-4-3  后验概率最大化的含义是什么？](#2-4-3)

- [ ] [2-4-4  朴素贝叶斯模型如何学习的？训练过程是怎样？](#2-4-4)

- [ ] [2-4-5  你如何理解生成模型和判别模型？](#2-4-5)

- [ ] [2-4-6  朴素贝叶斯模型“朴素”体现在哪里？存在什么问题？有哪些优化方向？](#2-4-6)

- [ ] [2-4-7  什么是贝叶斯网络？它能解决什么问题？](#2-4-7)

- [ ] [2-4-8  为什么说朴素贝叶斯也是线性模型而不是非线性模型呢？](#2-4-8)



**线性回归**

- [ ] [2-5-1  线性回归的基本思想是？](#2-5-1)

- [ ] [2-5-2  什么是“广义线性模型”？](#2-5-2)

- [ ] [2-5-3  线性回归常用的损失函数有哪些？优化算法有哪些？](#2-5-3)

- [ ] [2-5-4  线性回归适用什么类型的问题？有哪些优缺点？](#2-5-4)

- [ ] [2-5-5  请用最小二乘法推倒参数更新公式？](#2-5-5)



**逻辑回归**

- [ ] [2-6-1  逻辑回归相比于线性回归有什么异同？](#2-6-1)

- [ ] [2-6-2  逻辑回归和广义线性模型有何关系？](#2-6-2)

- [ ] [2-6-3  逻辑回归如何处理多标签分类？](#2-6-3)

- [ ] [2-6-4  为什么逻辑回归需要进行归一化或者取对数？](#2-6-4)

- [ ] [2-6-5  为什么逻辑回归把特征离散化之后效果会提升？](#2-6-5)

- [ ] [2-6-6  类别不平衡问题你是如何处理的？什么是过采样，什么是欠采样？举例](#2-6-6)

- [ ] [2-6-7  讲解L1和L2正则，它们都有什么作用，解释为什么L1比L2更容易产生稀疏解；对于存在线性相关的一组特征，L1正则如何选择特征？](#2-6-7)

- [ ] [2-6-8  使用交叉熵作为损失函数，梯度下降作为优化方法，推倒参数更新公式](#2-6-8)

- [ ] [2-6-9  代码写出训练函数](#2-6-9)



**FM模型**

- [ ] [2-7-1  FM模型与逻辑回归相比有什么优缺点？](#2-7-1)

- [ ] [2-7-2  为什么FM模型计算复杂度时O(kn)？](#2-7-2)

- [ ] [2-7-3  介绍FFM场感知分解机器（Field-aware Factorization Machine），说说与FM异同？](#2-7-3)

- [ ] [2-7-4  使用FM进行模型训练时候，有哪些核心参数对模型效果影响大？](#2-7-4)

- [ ] [2-7-5  如何从神经网络的视角看待FM模型？](#2-7-5)



**决策树**

- [x] [2-8-1  讲解完成的决策树的建树过程](#2-8-1)

- [x] [2-8-2  你是如何理解熵？从数学原理上解释熵公式可以作为信息不确定性的度量？](#2-8-2)

- [ ] [2-8-3  联合熵、条件熵、KL散度、信息增益、信息增益比、gini系数都是什么？如何计算？](#2-8-3)

- [x] [2-8-4  常用的决策树有哪些？ID3、C4.5、CART有啥异同？](#2-8-4)

- [ ] [2-8-5  决策树如何防止过拟合？前剪枝和后剪枝过程是怎样的？剪枝条件都是什么](#2-8-5)



**随机森林（RF）**

- [ ] [2-9-1  介绍RF原理和思想](#2-9-1)
- [ ] [2-9-2  RF是如何处理缺失值？](#2-9-2)
- [ ] [2-9-3  RF如何衡量特征重要度？](#2-9-3)
- [ ] [2-9-4  RF“随机”主要体现在哪里？](#2-9-4)
- [ ] [2-9-5  RF有哪些优点和局限性？](#2-9-5)
- [ ] [2-9-6  为什么多个弱分类器组合效果会比单个要好？如何组合弱分类器可以获得更好的结果？原因是什么？](#2-9-6)
- [x] [2-9-7  Bagging的思想是什么？它是降低偏差还是方差，为什么？](#2-9-7)
- [x] [2-9-8  可否将RF的基分类模型由决策树改成线性模型或者knn？为什么？](#2-9-8)



**GBDT**

- [ ] [2-10-1  梯度提升和梯度下降有什么区别和联系？](#2-10-1)

- [x] [2-10-2  你是如何理解Boosting和Bagging？他们有什么异同？](#2-10-2)

- [ ] [2-10-3  讲解GBDT的训练过程？](#2-10-3)

- [ ] [2-10-4  你觉得GBDT训练过程中哪些环节可以平行提升训练效率？](#2-10-4)

- [x] [2-10-5  GBDT的优点和局限性有哪些？](#2-10-5)

- [ ] [2-10-6  GBDT是否对异常值敏感，为什么？](#2-10-6)

- [ ] [2-10-7  如何防止GBDT过拟合？](#2-10-7)

- [ ] [2-10-8  在训练过程中哪些参数对模型效果影响比较大？这些参数造成影响是什么？](#2-10-8)



**k-means**

- [ ] [2-11-1  简述kmeans建模过程？](#2-11-1)

- [ ] [2-11-2  Kmeans损失函数是如何定义？](#2-11-2)

- [ ] [2-11-3  你是如何选择初始类族的中心点？](#2-11-3)

- [ ] [2-11-4  如何提升kmeans效率？](#2-11-4)

- [ ] [2-11-5  常用的距离衡量方法有哪些？他们都适用什么类型问题？](#2-11-5)

- [ ] [2-11-6  Kmeans对异常值是否敏感？为什么？](#2-11-6)

- [ ] [2-11-7  如何评估聚类效果？](#2-11-7)

- [ ] [2-11-8  超参数类的个数k如何选取？](#2-11-8)

- [ ] [2-11-9  Kmeans有哪些优缺点？是否有了解过改进的模型，举例说明？](#2-11-9)

- [ ] [2-11-10  试试证明kmeans算法的收敛性](#2-11-10)

- [ ] [2-11-11  除了kmeans聚类算法之外，你还了解哪些聚类算法？简要说明原理](#2-11-11)



**PCA降维**

- [ ] [2-12-1  为什么要对数据进行降维？它能解决什么问题？](#2-12-1)

- [ ] [2-12-2  你是如何理解维度灾难？](#2-12-1)

- [ ] [2-12-3  PCA主成分分析思想是什么？](#2-12-1)

- [ ] [2-12-4  如何定义主成分？](#2-12-1)

- [ ] [2-12-5  如何设计目标函数使得降维达到提取主成分的目的？](#2-12-1)

- [ ] [2-12-6  PCA有哪些局限性？如何优化](#2-12-1)

- [ ] [2-12-7  线性判别分析和主成分分析在原理上有何异同？在目标函数上有何区别和联系？](#2-12-1)



**3、 深度学习**

**DNN**

- [ ] [3-1-1  描述一下神经网络？推倒反向传播公式？](#3-1-1)

- [x] [3-1-2  讲解一下dropout原理？](#3-1-2)

- [ ] [3-1-3  梯度消失和梯度膨胀的原因是什么？有什么方法可以缓解？](#3-1-3)

- [ ] [3-1-4  什么时候该用浅层神经网络，什么时候该选择深层网络](#3-1-4)

- [ ] [3-1-5  Sigmoid、Relu、Tanh激活函数都有哪些优缺点？](#3-1-5)

- [ ] [3-1-6  写出常用激活函数的导数](#3-1-6)

- [ ] [3-1-7  训练模型的时候，是否可以把网络参数全部初始化为0？为什么](#3-1-7)

- [ ] [3-1-8  Batchsize大小会如何影响收敛速度？](#3-1-8)



3-1-1



**CNN**

- [ ] [3-2-1  简述CNN的工作原理？](#3-2-1)

- [ ] [3-2-2  卷积核是什么？选择大卷积核和小卷积核有什么影响？](#3-2-2)

- [ ] [3-2-3  你在实际应用中如何设计卷积核？](#3-2-3)

- [ ] [3-2-4  为什么CNN具有平移不变性？](#3-2-4)

- [ ] [3-2-5  Pooling操作是什么？有几种？作用是什么？](#3-2-5)

- [ ] [3-2-6  为什么CNN需要pooling操作？](#3-2-6)

- [ ] [3-2-7  什么是batchnormalization？它的原理是什么？在CNN中如何使用？](#3-2-7)

- [ ] [3-2-8  卷积操作的本质特性包括稀疏交互和参数共享，具体解释这两种特性以其作用？](#3-2-8)

- [ ] [3-2-9  你是如何理解fine-tune？有什么技巧](#3-2-9)

 

**RNN**

- [ ] [3-3-1  简述RNN模型原理，说说RNN适合解决什么类型问题？为什么](#3-3-1)

- [ ] [3-3-2  RNN和DNN有何异同？](#3-3-2)

- [ ] [3-3-3  RNN为什么有记忆功能？](#3-3-3)

- [ ] [3-3-4  长短期记忆网络LSTM是如何实现长短期记忆功能的？](#3-3-4)

- [ ] [3-3-5  长短期记忆网络LSTM各模块都使用什么激活函数，可以使用其他激活函数么？](#3-3-5)

- [ ] [3-3-6  GRU和LSTM有何异同](#3-3-6)

- [ ] [3-3-7  什么是Seq2Seq模型？该模型能解决什么类型问题？](#3-3-7)

- [ ] [3-3-8  注意力机制是什么？Seq2Seq模型引入注意力机制主要解决什么问题？](#3-3-8)


**4、 基础工具**

**Spark**

- [ ] [4-1-1  什么是宽依赖，什么是窄依赖？哪些算子是宽依赖，哪些是窄依赖？](#4-1-1)

- [ ] [4-1-2  Transformation和action算子有什么区别？举例说明](#4-1-2)

- [ ] [4-1-3  讲解sparkshuffle原理和特性？shuffle write 和 huffleread过程做些什么？](#4-1-1)

- [ ] [4-1-4  哪些spark算子会有shuffle？](#4-1-1)

- [ ] [4-1-5  讲解sparkschedule（任务调度）？](#4-1-1)

- [ ] [4-1-6  Sparkstage是如何划分的？](#4-1-1)

- [ ] [4-1-7  Sparkcache一定能提升计算性能么？说明原因？](#4-1-1)

- [ ] [4-1-8  Cache和persist有什么区别和联系？](#4-1-1)

- [ ] [4-1-9  RDD是弹性数据集，“弹性”体现在哪里呢？你觉得RDD有哪些缺陷？](#4-1-1)

- [ ] [4-1-10  当GC时间占比很大可能的原因有哪些？对应的优化方法是？](#4-1-1)

- [ ] [4-1-11  park中repartition和coalesce异同？coalesce什么时候效果更高，为什么](#4-1-1)

- [ ] [4-1-12  Groupbykey和reducebykey哪个性能更高，为什么？](#4-1-1)

- [ ] [4-1-13  你是如何理解caseclass的？](#4-1-1)

- [ ] [4-1-14  Scala里trait有什么功能，与class有何异同？什么时候用trait什么时候用class](#4-1-1)

- [ ] [4-1-15  Scala 语法中to 和 until有啥区别](#4-1-1)

- [ ] [4-1-16  讲解Scala伴生对象和伴生类](#4-1-1)





**Xgboost**

- [ ] [4-2-1  你选择使用xgboost的原因是什么？](#4-2-1)

- [ ] [4-2-2  Xgboost和GBDT有什么异同？](#4-2-2)

- [ ] [4-2-3  为什么xgboost训练会那么快，主要优化点事什么？](#4-2-3)

- [ ] [4-2-4  Xgboost是如何处理缺失值的？](#4-2-4)

- [ ] [4-2-5  Xgboost和lightGBM有哪些异同？](#4-2-5)

- [ ] [4-2-6  Xgboost为什么要使用泰勒展开式，解决什么问题？](#4-2-6)

- [ ] [4-2-7  Xgboost是如何寻找最优特征的？](#4-2-7)



**Tensorflow**

- [ ] [4-3-1  使用tensorflow实现逻辑回归，并介绍其计算图](#4-3-1)

- [ ] [4-3-2  sparse_softmax_cross_entropy_with_logits和softmax_cross_entropy_with_logits有何异同？](#4-3-2)

- [ ] [4-3-3  使用tensorflow过程中，常见调试哪些参数？举例说明](#4-3-3)

- [ ] [4-3-4  Tensorflow梯度更新是同步还是异步，有什么好处？](#4-3-4)

- [ ] [4-3-5  讲解一下TFRecords](#4-3-5)

- [ ] [4-3-6  tensorflow如何使用如何实现超大文件训练？](#4-3-6)

- [ ] [4-3-7  如何读取或者加载图片数据？](#4-3-7)



**5、推荐系统**

- [ ] [5-1-1  你是如何选择正负样本？如何处理样本不均衡的情况？](#5-1-1)

- [ ] [5-1-2  如何设计推荐场景的特征体系？举例说明](#5-1-1)

- [ ] [5-1-3  你是如何建立用户模型来理解用户，获取用户兴趣的？](#5-1-1)

- [ ] [5-1-4  你是如何选择适合该场景的推荐模型？讲讲你的思考过程](#5-1-1)

- [ ] [5-1-5  你是如何理解当前流行的召回->粗排->精排的推荐架构？这种架构有什么优缺点？什么场景适用使用，什么场景不适合？](#5-1-1)

- [ ] [5-1-6  如何解决热度穿透的问题？（因为item热度非常高，导致ctr类特征主导排序，缺少个性化的情况）](#5-1-1)

- [ ] [5-1-7  用户冷启动你是如何处理的？](#5-1-1)
- [ ] [5-1-8  新内容你是如何处理的？](#5-1-1)

- [ ] [5-1-9  你们使用的召回算法有哪些？如何能保证足够的召回率？](#5-1-1)

- [ ] [5-1-10  实时数据和离线数据如何融合？工程上是怎样实现？如何避免实时数据置信度不高带来的偏差问题？](#5-1-1)

- [ ] [5-1-11  你们是如何平衡不同优化目标的问题？比如：时长、互动等](#5-1-1)

- [ ] [5-1-12  不同类型内容推荐时候，如何平衡不同类型内容，比如图文、视频；或者不同分类](#5-1-1)

- [ ] [5-1-13  如何保证线上线下数据一致性？工程上是如何实现？](#5-1-1)

- [ ] [5-1-14  离线训练效果好，但是上线效果不明显或在变差可能是什么问题？如何解决？](#5-1-1)

- [ ] [5-1-15  在实际业务中，出现badcase,你是如何快速反查问题的？举例说明](#5-1-1)

- [ ] [5-1-16  使用ctr预估的方式来做精排，会不会出现相似内容大量聚集？原因是什么？你是如何解决的？](#5-1-1)

- [ ] [5-1-17  你了解有多少种相关推荐算法？各有什么优缺点](#5-1-1)

- [ ] [5-1-18  深度学习可以应用到推荐问题上解决哪些问题？为什么比传统机器学习要好？](#5-1-1)



**二、数学相关**

**6、 概率论和统计学**

- [ ] [6-1-1  说说你是怎样理解信息熵的？](#6-1-1)

- [ ] [6-1-2   能否从数据原理熵解析信息熵可以表示随机变量的不确定性？](#6-1-2)

- [ ] [6-1-3  怎样的模型是最大熵模型？它有什么优点](#6-1-3)

- [ ] [6-1-4  什么是Beta分布？它与二项分布有什么关系？](#6-1-4)

- [ ] [6-1-5   什么是泊松分布？它与二项分布有什么关系？](#6-1-5)

- [ ] [6-1-6  什么是t分布？他与正态分布有什么关系？](#6-1-6)

- [ ] [6-1-7    什么是多项式分布？具体说明？](#6-1-7)

- [ ] [6-1-8   参数估计有哪些方法？](#6-1-8)

- [ ] [6-1-9  点估计和区间估计都是什么？](#6-1-9)

- [ ] [6-1-10  讲解一下极大似然估计，以及适用场景？](#6-1-10)



7、 最优化问题

- [ ] [7-1-1  什么是梯度？](#7-1-1)

- [ ] [7-1-2  梯度下降找到的一定是下降最快的方法？](#7-1-1)

- [ ] [7-1-3  牛顿法和梯度法有什么区别？](#7-1-1)

- [ ] [7-1-4  什么是拟牛顿法？](#7-1-1)

- [ ] [7-1-5  讲解什么是拉格朗日乘子法、对偶问题、kkt条件?](#7-1-1)

- [ ] [7-1-6  是否所有的优化问题都可以转化为对偶问题？](#7-1-1)

- [ ] [7-1-7  讲解SMO（SequentialMinimalOptimization）算法基本思想？](#7-1-1)

- [ ] [7-1-8  为什么深度学习不用二阶优化？](#7-1-1)

- [ ] [7-1-9  讲解SGD，ftrl、Adagrad，Adadelta，Adam，Adamax，Nadam优化算法以及他们的联系和优缺点](#7-1-1)

- [ ] [7-1-10 为什么batch size大，训练速度快or为什么mini-batch比SGD快？](#7-1-10)

- [ ] [7-1-11  为什么不把batch size设得特别大](#7-11)









# 解答

## **一、机器学习相关**

## **1、 基本概念**

- [ ] <span id="1-1">1-1 简述解决一个机器学习问题时，你的流程是怎样的？</span>

  1. 确定问题：有监督问题还是无监督问题？回归问题还是分类问题？
  2. 数据收集与处理：
  3. 特征工程：包括特征构建、特征选择、特征组合等
  4. 模型训练、调参、评估：包括模型的选择，选择最优的参数
  5. 模型部署：模型在线上运行的效果直接决定模型的成败

- [ ] [1-2  损失函数是什么，如何定义合理的损失函数？](#1-2)

  机器学习模型关于单个样本的预测值与真实值的差称为**损失**。用于计算损失的函数称为**损失函数**。

- [ ] [1-3  回归模型和分类模型常用损失函数有哪些？各有什么优缺点](#1-3)

  回归模型常用的损失函数有：

  1. 绝对损失函数：异常点多的情况下鲁棒性好；但不方便求导
     $$
     L(f, y) = |f-y|
     $$

  2. 平方损失函数：求导方便，能够用梯度下降法优化；对异常值敏感
     $$
     L(f,y) = (f-y)^2
     $$

  3. Huber 损失函数：结合了上述两个损失函数的优点；缺点是需要调整超参数  \delta 
     $$
     L_{Huber}(f, y) =
     \begin{cases}
     (f-y)^2
     &
     |f-y| \leq \delta
     \\
     2 \delta |f-y| - \delta^2
     &
     |f-y| > \delta
     \end{cases}
     $$

  4. Log-Cosh 损失函数：具有Huber的所有优点，同时二阶处处可微（牛顿法要求二阶可微）
     $$
     L(f,y) = \log \cosh(f-y)
     $$

- [ ] [1-4  什么是结构误差和经验误差？训练模型的时候如何判断已经达到最优？](#1-4)

- [ ] [1-5  模型的“泛化”能力是指？如何提升模型泛化能力？](#1-5)

- [ ] [1-6  如何选择合适的模型评估指标？AUC、精准度、召回率、F1值都是什么？如何计算？有什么优缺点？](#1-6)

**Accuracy**（准确率）：分类正确的样本占总样本个数的比例
$$
Accuracy = \frac{n_{correct}}{n_{total}}
$$
- 缺点：不同类别的样本比例非常不均衡时，占比大的类别往往成为影响准确率的最主要因素。比如，当负样本占99%时，分类器把所有样本都预测为负样本也可以获得99%的准确率。
- 解决：可以使用每个类别下的样本准确率的算术平均（平均准确率）作为模型评估的指标。

**Precision**（精确率）：分类正确的正样本个数占分类器判定为正样本的样本个数的比例

**Recall**（召回率）：分类正确的正样本数占真正的正样本个数的比例

**F1-score**：precision和recall的调和平均值
$$
{\rm F1} = \frac{2 \times precision \times recall}{precision + recall}
$$
在排序问题中，通常没有一个确定的阈值把得到的结果直接判定为正样本或负样本，而是采用Top N返回结果的Precision和Recall值来衡量排序模型的性能。即认为模型返回的Top N结果就是模型判定的正样本，计算前N个位置的Precision@N和Recall@N。为了综合评估一个排序模型的好坏，不仅要看模型在不同Top N下的Precision@N和Recall@N，而且最好画出模型的P-R曲线。P-R曲线的横轴是Recall，纵轴是Precision。

**ROC**：横坐标为假阳性率（False Positive Rate，FPR）；纵坐标为真阳性率（True Positive Rate，TPR）
$$
FPR = \frac{FP}{N} \\
TPR = \frac{TP}{P}
$$
其中P是真实的正样本的数量，N是真实的负样本的数量，TP是P个正样本中被分类器预测为正样本的个数，FP是N个负样本中被预测为正样本的个数。

【如何绘制ROC曲线】通过不断移动分类器的“截断点”来生成曲线上的一组关键点。在二分类问题中，模型输出一般是预测样本为正例的概率，在输出最终的正例负例之前，我们需要制定一个阈值。大于该阈值的样本判定为正例，小于该阈值的样本判定为负例。通过动态调整截断点，绘制每个截断点对应位置，再连接所有点得到最终的ROC曲线。

**AUC**：ROC曲线下的面积大小。计算AUC值只要沿着ROC横轴做积分就可以。AUC取值一般在0.5~1之间。AUC越大，分类性能越好。



- [ ] [1-7  什么是混淆矩阵？](#1-7)

  混淆矩阵，又称误差矩阵，就是分别统计分类模型归错类，归对类的观测值个数，然后把结果放在一个表里展示出来。这个表就是混淆矩阵。

  混淆矩阵是ROC曲线绘制的基础，同时它也是衡量分类型模型准确度中最基本，最直观，计算最简单的方法。

   

  | 混淆矩阵 |   预测结果   |   预测结果   |
  | :------: | :----------: | :----------: |
  | 真实情况 |     反例     |     正例     |
  |   反例   | TN（真反例） | FP（假正例） |
  |   正例   | FN（假反例） | TP（真正例） |


  - TN：True Negative，被判定为负样本，事实上也是负样本

  - FP：False Positive，被判定为正样本，但事实上是负样本

  - FN：False Negative，被判定为负样本，但事实上是正样本

  - TP：True Positive，被判定为正样本，事实上也是正样本

- [ ] [1-8  ROC曲线如何绘制？相比P-R曲线有什么特点？](#1-8)

  ROC曲线的纵轴为TPR，横轴为FPR，曲线上每个点对应一个TPR和FPR。通过调整模型阈值可以得到一个个点，从而将各个点连起来即可得到ROC曲线。

  一般情况下，PR曲线易受样本数量的影响，样本数量不均衡情况下PR曲线会有明显变化，故一般使用ROC曲线。

- [ ] [1-9  如何评判模型是过拟合还是欠拟合？遇到过拟合或欠拟合时，你是如何解决？](#1-9)

当训练集效果差，欠拟合（如accuracy<0.8）；训练集效果好，测试集效果差，过拟合

欠拟合解决方法：

1. 增加特征
2. 提高模型复杂度：神经网络提高神经元数、增加层数；SVM使用核函数；
3. 减小正则项的系数

过拟合解决方法：

1. 提高样本数量 ：
   - 神经网络：Data Augmentation（数据增强）
2. 简化模型：
   - 神经网络使用 Dropout、Early Stopping
   - 决策树剪枝、限制树的深度
3. 加入正则化项（L1或L2）或提高惩罚系数
4. 使用集成学习



- [ ] [1-10  你是如何针对应用场景选择合适的模型？](#1-10)
- [ ] [1-11  如何选择模型中的超参数？有什么方法，并说说其优劣点](#1-11)

超参搜索算法一般包括的要素（1）目标函数（2）搜索范围，上限和下限缺点（3）其他参数，如搜索步长。

1. **网格搜索**

查找搜索范围内所有的点来确定最优值；实际应用中先用较大搜索范围和较大步长，寻找全局最优值可能位置；然后逐步缩小搜索范围和搜索步长，寻找更精确位置。

- 优点
  - 简单
  - 如果采用较大的搜索范围和较小步长，有很大概率找到全局最优值
- 缺点
  - 耗时
  - 目标函数非凸时，可能错过全局最优解

2. **随机搜索**

不再测试上界和下界之间的所有值，而是在搜索范围中随机选取样本点。如果样本点集足够大，通过随机搜索也能大概率找到全局最优解或其近似。

- 优点
  - 更快
- 缺点
  - 可能错过全局最优解

3. **贝叶斯优化算法**

对目标函数形状进行学习，找到使目标函数向全局最优值提升的参数。

- 优点
  - 不同于前两种方法测试一个新点时会忽略前一个点的信息；贝叶斯优化算法充分利用之前的信息

- 缺点
  - 容易陷入局部最优值



- [ ] [1-12  误差分析是什么？你是如何进行误差分析？](#1-12)
- [ ] [1-13  你是如何理解模型的偏差和方差？什么样的情况是高偏差，什么情况是高方差？](#1-13)
- [ ] [1-14  出现高偏差或者高方差的时候你有什么优化策略？](#1-14)
- [ ] [1-15  奥卡姆剃刀定律是什么？对机器学习模型优化有何启发？举例说明](#1-15)
- [ ] [1-16  线性模型和非线性模型的区别？哪些模型是线性模型，哪些模型是非线性模型？](#1-16)
- [ ] [1-17  生成式模型和判别式模型的区别？哪些模型是生成式模型，哪些模型是判别式模型？](#1-17)



## **2、经典机器学习**

### **特征工程**

- [ ] [2-1-1  你是怎样理解“特征”？](#2-1-1)
- [ ] [2-1-2  给定场景和问题，你如何设计特征？（特征工程方法论）](#2-1-2)
- [ ] [2-1-3  开发特征时候做如何做数据探索，怎样选择有用的特征？](#2-1-3)
- [ ] [2-1-4  你是如何做数据清洗的？举例说明](#2-1-4)
- [ ] [2-1-5  如何发现数据中的异常值，你是如何处理？](#2-1-5)
- [ ] [2-1-6  缺失值如何处理？](#2-1-6)
- [ ] [2-1-7  对于数值类型数据，你会怎样处理？为什么要做归一化？归一化有哪些方法？离散些方法，离散化和归一化有哪些优缺点？](#2-1-7)
- [ ] [2-1-8  标准化和归一化异同？](#2-1-8)
- [ ] [2-1-9  你是如何处理CTR类特征？](#2-1-9)
- [ ] [2-1-10  讲解贝叶斯平滑原理？以及如何训练得到平滑参数](#2-1-10)
- [ ] [2-1-11  类别型数据你是如何处理的？比如游戏品类，地域，设备](#2-1-11)
- [ ] [2-1-12  序号编码、one-hot编码、二进制编码都是什么？适合怎样的类别型数据？](#2-1-12)
- [ ] [2-1-13  时间类型数据你的处理方法是什么？原因？](#2-1-13)
- [ ] [2-1-14  你怎样理解组合特征？举个例子，并说明它和单特征有啥区别](#2-1-14)
- [ ] [2-1-15  如何处理高维组合特征？比如用户ID和内容ID？](#2-1-15)
- [ ] [2-1-16  如何理解笛卡尔积、外积、内积？](#2-1-16)
- [ ] [2-1-17  文本数据你会如何处理？](#2-1-17)
- [ ] [2-1-18  文本特征表示有哪些模型？他们的优缺点都是什么？](#2-1-18)

**词袋模型**（Bag of Words）：每篇文章看成一袋子词，并忽略每个词出现的顺序。每篇文章可以表示成一个长向量，向量中的每一位代表一个单词，该维对应的权重则反映这个词在原文章中的重要程度，常用TF-IDF来计算权重。

缺点：无法识别出两个不同的词或者词组具有相同的主题。

**N-gram模型**：将连续出现的n个词（n<=N）组成的词组也作为一个单独的特征放到向量表示中。

缺点：无法识别出两个不同的词或者词组具有相同的主题。

**主题模型**（Topic Model）：是一种特殊的概率图模型

**词嵌入模型**（Word Embedding）：将词向量化；核心思想是将每个词映射到低维空间（K=50~300维）上的一个稠密向量。K维空间的每一维也可以看作一个隐含的主题



- [ ] [2-1-19  讲解TFF原理，它有什么优点和缺点？针对它的缺点，你有什么优化思路？](#2-1-19)

$$
{\rm TF-IDF}(t,d) = {\rm TF}(t,d) \times {\rm IDF}(t)
$$

其中 ${\rm TF}(t,d)$ 为单词t在文档d中出现的频率，${\rm IDF}(t)$ 是逆文档频率，用来衡量单词t对表达语义所起的重要性
$$
{\rm IDF}(t) = {\rm log} \frac{文章总数}{包含单词t的文章总数+1}
$$
优点：



- [ ] [2-1-20  N-gram算法是什么？有什么优缺点？](#2-1-20)
- [ ] [2-1-21  讲解一下word2vec工作原理？损失函数是什么？](#2-1-21)
- [ ] [2-1-22  讲解一下LDA模型原理和训练过程？](#2-1-22)
- [ ] [2-1-23  Word2vec和LDA两个模型有什么区别和联系？](#2-1-23)
- [ ] [2-1-24  Skin-gram和cbow有何异同？](#2-1-24)

不同点：skip-gram使用中心词预测上下文；CBOW用上下文预测中心词

相同点：都是根据word共现的频率建模word的相似度



- [ ] [2-1-25  图像数据如何处理？有哪些常用的图像特征提取方法](#2-1-25)
- [ ] [2-1-26  你是怎样做特征选择的？卡方检验、信息值（IV）、VOE都是如何计算？](#2-1-26)
- [ ] [2-1-27  计算特征之间的相关性方法有哪些？有什么优缺点](#2-1-27)



### **基础算法原理和推导**

#### **KNN**

- [ ] [2-2-1  Knn建模流程是怎样的？](#2-2-1)

- [ ] [2-2-2  Knn优缺点是什么？](#2-2-2)

- [ ] [2-2-3  Knn适合什么样的场景和数据类型？](#2-2-3)

- [ ] [2-2-4  常用的距离衡量公式都有哪些？具体说明它们的计算流程，以及使用场景？](#2-2-4)

  1.**欧式距离**

  强调数值上的绝对误差

  是严格定义的距离，满足正定性、对称性、三角不等式
  $$
  d(x,y) = \sqrt{\sum_{i=1}^{N} (x_i - y_i)^2}
  $$
  2.**余弦相似度**

  强调方向上的相对误差

  不是严格定义的距离，满足正定性、对称性，不满足三角不等式

$$
cos(A,B) = \frac{A \cdot B}{||A||_2 ||B||_2}
$$

3. **KL散度**

   计算两个分布的差异性

   不是严格定义的距离，满足正定性，不满足对称性、三角不等式



- [ ] [2-2-5  超参数K值过大或者过小对结果有什么影响，你是如何选择K值？](#2-2-5)

- [ ] [2-2-6  介绍一下Kd树？如何建树，以及如何搜索最近节点？](#2-2-6)

  ·

#### **支持向量机**

- [ ] [2-3-1  简单讲解SVM模型原理？](#2-3-1)
- [ ] [2-3-2  SVM为什么会对缺失值敏感？实际应用时候你是如何处理？](#2-3-2)
- [ ] [2-3-3  SVM为什么可以分类非线性问题？](#2-3-3)
- [ ] [2-3-4  常用的核函数有哪些？你是如何选择不同的核函数的？](#2-3-4)

**高斯核函数** RBF kernel
$$
K(x,z) = exp(-\frac{1}{2} \ ||x - z ||_2 ) = \phi(x) \cdot \phi(z)
$$

$$
f(x) = {\rm sign} ()
$$



**Sigmod核函数**



**字符串核函数**

​	在一些结构化的数据，如字符串序列中，序列长度未知，难以设计 $\phi(x)$。$K(x,z)$ 类似相似度，我们使用SVM时只要定K的形式，去计算序列的相似度。





- [ ] [2-3-5  RBF核函数一定线性可分么？为什么](#2-3-5)
- [ ] [2-3-6  SVM属于线性模型还是非线性模型？为什么？](#2-3-6)
- [ ] [2-3-7  训练误差为0的SVM分类器一定存在吗？说明原因？](#2-3-7)



#### **朴素贝叶斯模型**

- [ ] [2-4-1  讲解贝叶斯定理？](2-4-1)
- [ ] [2-4-2  什么是条件概率、边缘概率、联合概率？](#2-4-2)
- [ ] [2-4-3  后验概率最大化的含义是什么？](#2-4-3)
- [ ] [2-4-4  朴素贝叶斯模型如何学习的？训练过程是怎样？](#2-4-4)
- [ ] [2-4-5  你如何理解生成模型和判别模型？](#2-4-5)
- [ ] [2-4-6  朴素贝叶斯模型“朴素”体现在哪里？存在什么问题？有哪些优化方向？](#2-4-6)
- [ ] [2-4-7  什么是贝叶斯网络？它能解决什么问题？](#2-4-7)
- [ ] [2-4-8  为什么说朴素贝叶斯也是线性模型而不是非线性模型呢？](#2-4-8)



#### **线性回归**

- [ ] [2-5-1  线性回归的基本思想是？](#2-5-1)
- [ ] [2-5-2  什么是“广义线性模型”？](#2-5-2)
- [ ] [2-5-3  线性回归常用的损失函数有哪些？优化算法有哪些？](#2-5-3)
- [ ] [2-5-4  线性回归适用什么类型的问题？有哪些优缺点？](#2-5-4)
- [ ] [2-5-5  请用最小二乘法推倒参数更新公式？](#2-5-5)



#### **逻辑回归**

- [ ] [2-6-1  逻辑回归相比于线性回归有什么异同？](#2-6-1)

**不同点**：

逻辑回归处理的是分类问题，线性回归处理的是回归问题；

逻辑回归中认为y是因变量，即逻辑回归的因变量是离散的，线性回归的因变量是连续的。



**相同点：**

二者都使用了极大似然估计来对训练样本进行建模

求解超参数过程中，都可以使用梯度下降的方法



**联系**：

如果把一个事件的几率（odds）定义为该事件发生的概率与不发生概率的比值 $\frac{p}{1-p}$ ，那么逻辑回归可以看做是对于"y=1|x"这一事件的对数几率的线性回归
$$
{\rm log} \frac{p}{1-p} = \theta^{T}x ，其中\ p  = P(y=1|x)
$$


- [ ] [2-6-2  逻辑回归和广义线性模型有何关系？](#2-6-2)

可以看做广义线性模型在因变量y服从二元分布时的一个特殊情况


- [ ] [2-6-3  逻辑回归如何处理多标签分类？](#2-6-3)

如果一个样本只对应于一个标签（多分类问题）：
假设每个样本属于不同标签的概率服从几何分布，使用softmax regression进行分类：
$$
h_\theta =
 \left[
 \begin{matrix}
   p(y=1|x;\theta)\\
   p(y=2|x;\theta) \\
   \vdots \\
   p(y=1|x;\theta)
  \end{matrix}
  \right] 
  = 
  \frac{1}{\sum_{j=1}^{k} e^{\theta^T x}}
  
  \left[
 \begin{matrix}
   e^{\theta_1^T x}\\
   e^{\theta_2^T x} \\
   \vdots \\
   e^{\theta_k^T x}
  \end{matrix}
  \right] 
  
  \tag{3}
$$
其中 $\theta_1,\theta_2 \dots,\theta_k \in \mathbb{R}^n​$

如果存在样本可能属于多个标签的情况时，可以训练k个二分类的逻辑回归分类器。第i个分类器用以区分每个样本是否可以归为第i类。



- [ ] [2-6-4  为什么逻辑回归需要进行归一化或者取对数？](#2-6-4)

- [ ] [2-6-5  为什么逻辑回归把特征离散化之后效果会提升？](#2-6-5)

- [ ] [2-6-6  类别不平衡问题你是如何处理的？什么是过采样，什么是欠采样？举例](#2-6-6)

- [ ] [2-6-7  讲解L1和L2正则，它们都有什么作用，解释为什么L1比L2更容易产生稀疏解；对于存在线性相关的一组特征，L1正则如何选择特征？](#2-6-7)

L1和L2正则，都可以防止过拟合，增强模型的泛化能力；区别在于L1使参数更稀疏，达到特征选取的作用；L2使参数更接近于0

从解空间的形状来看：

L1正则项约束后的解空间是多边形，而L2正则项约束后的解空间是圆形。而多边形的解空间更容易在尖角处与等高线碰撞出稀疏解。

对于存在线性相关的一组特征，L1正则会使得部分参数为0



- [ ] [2-6-8  使用交叉熵作为损失函数，梯度下降作为优化方法，推倒参数更新公式](#2-6-8)
- [ ] [2-6-9  代码写出训练函数](#2-6-9)



#### **FM模型**

- [ ] [2-7-1  FM模型与逻辑回归相比有什么优缺点？](#2-7-1)
- [ ] [2-7-2  为什么FM模型计算复杂度时O(kn)？](#2-7-2)
- [ ] [2-7-3  介绍FFM场感知分解机器（Field-aware Factorization Machine），说说与FM异同？](#2-7-3)
- [ ] [2-7-4  使用FM进行模型训练时候，有哪些核心参数对模型效果影响大？](#2-7-4)
- [ ] [2-7-5  如何从神经网络的视角看待FM模型？](#2-7-5)



#### **决策树**

- [x] [2-8-1  讲解完成的决策树的建树过程](#2-8-1)

自上而下，对样本数据进行树形分类的过程。每个内部节点表示一个特征，叶节点表示一个类别。从顶部根节点开始，所有的样本聚在一起。经过根节点的划分，样本被分到不同的子节点，再根据子节点的特征进一步划分，直至所有样本被归到某一个类别（叶节点）中。



- [x] [2-8-2  你是如何理解熵？从数学原理上解释熵公式可以作为信息不确定性的度量？](#2-8-2)

熵（entropy）是表示随机变量不确定性的度量， $X$ 是一个取有限个值的离散随机变量，其概率分布为
$$
P(X = x_i) = p_i, \ i=1,2,\cdots,n
$$
则随机变量 $X$ 的熵定义为
$$
H(X) = \sum_{i=1}^{n} p_i {\rm log } \ p_i 
$$
熵越大，随机变量的不确定性就越大。



而熵其实表示的是一个系统的平均信息量。**自信息量**是用来描述某一条信息的大小
$$
I = - {\rm log} \ p_i
$$
通常以2为底，单位是bit；含义是用多少位二进制可以表示衡量该信息的大小。而通常我们衡量整个系统的信息量，系统存在多个事件 $X=\{x_1,\cdots,x_n\}$ ，每个事件的概率分布$P=\{p_1,\cdots,p_n\}$ ，**熵是整个系统的平均信息量** 。



- [ ] [2-8-3  联合熵、条件熵、KL散度、信息增益、信息增益比、gini系数都是什么？如何计算？](#2-8-3)

**联合熵**：

**条件熵**：某个特征A对于数据集D的经验条件熵 $H(D|A)$ 为
$$
H(D|A) = - \sum_{i=1}^{n} \frac{|D_i|}{|D|} H(D_i) \\ = - \sum_{i=1}^{n} \frac{|D_i|}{|D|} \lgroup \sum_{k=1}^{K} \frac{|D_{ik}|}{|D_i|} {\rm log } \frac{|D_{ik}|}{|D_i|} \rgroup
$$
**信息增益**： $g(D,A)$ 定义为数据集D的经验熵 $H(D)$ 与特征A给定条件下D的经验条件熵 $H(D|A)$ 的差
$$
g(D,A) = H(D) - H(D|A)g(D,A) = H(D) - H(D|A)
$$
**信息增益比**：特征A对于数据集D 的信息增益比定义为
$$
g_R(D|A) = \frac{g(D|A)}{H_A(D)}
$$
其中 
$$
H_A{(D)} = - \sum_{i=1}^{n} \frac{|D_i|}{|D|} {\rm log } \frac{|D_i|}{|D|}
$$
为数据集D关于A的取值熵；n为特征A在D上的取值数目；



**Gini系数**：描述数据的纯度。数据集D的Gini系数为
$$
{\rm Gini}(D) = 1 - \sum_{k=1}^{K
}(\frac{|C_k|}{|D|})^2
$$
其中 $C_k$是 D中第k类的样本子集，K是类的个数。



- [x] [2-8-4  常用的决策树有哪些？ID3、C4.5、CART有啥异同？](#2-8-4)

| 不同点   | ID3                  | C4.5               | CART                 |
| -------- | -------------------- | ------------------ | -------------------- |
| 用途     | 分类                 | 分类               | 分类、回归           |
| 输入取值 | 离散                 | 离散、连续         | 离散、连续           |
| 树结构   | 多叉树               | 多叉树             | 二叉树               |
|          | 特征在层级间不复用   | 特征在层级间不复用 | 每个特征可被重复利用 |
|          | 对样本特征缺失值敏感 |                    |                      |



**ID3	最大信息增益**

信息增益 $g(D,A)$ 定义为数据集D的经验熵 $H(D)$ 与特征A给定条件下D的经验条件熵 $H(D|A)$ 的差
$$
g(D,A) = H(D) - H(D|A)
$$
选择 $g(D,A)$  最大的特征，所有样本根据此特征，划分到不同的节点上。在经验熵不为0的节点中继续生长。ID3算法只有树的生成，容易产生过拟合。



**C4.5	最大信息增益比**

因为信息增益对取值数目多的属性有所偏好，为了减少这种偏好带来的影响，是一个信息增益比来选择最优划分属性。



**CART	基尼指数**

Gini用来描述数据的纯度，越小纯度越高。CART在每一次迭代中选择划分后**基尼指数最小**的特征及其对应的切分点进行分类。CART是一颗二叉树，每次将数据按特征A的区分分成两份，分别进入左右子树。



- [ ] [2-8-5  决策树如何防止过拟合？前剪枝和后剪枝过程是怎样的？剪枝条件都是什么](#2-8-5)

通过剪枝防止过拟合。

**预剪枝**是指在决策树生成的过程中，对每个节点在划分前先进行估计，若当前节点的划分不能带来决策树泛化性能提升，则停止划分，并将当前节点标记为叶子节点；

**后剪枝**则是先从训练集生成一棵完整的决策树，然后自底向上地对非叶子节点进行考察，若该节点对应的子树替换成叶子结点能带来泛化性能提升，则将该子树替换为叶子节点。





#### **随机森林（RF）**

- [ ] [2-9-1  介绍RF原理和思想](#2-9-1)
- [ ] [2-9-2  RF是如何处理缺失值？](#2-9-2)
- [ ] [2-9-3  RF如何衡量特征重要度？](#2-9-3)
- [ ] [2-9-4  RF“随机”主要体现在哪里？](#2-9-4)
- [ ] [2-9-5  RF有哪些优点和局限性？](#2-9-5)
- [ ] [2-9-6  为什么多个弱分类器组合效果会比单个要好？如何组合弱分类器可以获得更好的结果？原因是什么？](#2-9-6)
- [ ] [2-9-7  Bagging的思想是什么？它是降低偏差还是方差，为什么？](#2-9-7)

Bagging的思想是通过对数据再抽样，然后在每组样本上训练出来的模型取平均。Bagging是降低方差，防止过拟合。可以这样理解，对n个独立不想管的模型的预测结果取平均，方差是原来单个模型的 $1/n$ 。



- [ ] [2-9-8  可否将RF的基分类模型由决策树改成线性模型或者knn？为什么？](#2-9-8)

随机森林属于bagging类的集成学习方法，主要好处是减小集成后分类器的方差，比基分类器的方差小。所以Bagging所采用的的基分类器最好是本身对样本分布较为敏感（不稳定分类器），这样bagging才能体现效果。而线性分类器和KNN属于较为稳定的分类器，本身方差不大，所以将他们作为基分类器使用bagging不能再原基分类器的基础上获得更好的表现。相反地，可能因为bagging的采样而使得训练中难以收敛从而增大集成分类器的偏差。



#### **GBDT**

Gradient Boosting的基本思想是根据当前模型损失函数**负梯度**信息来训练新加入的弱分类器，然后将训练好的弱分类器以**累加**的形式结合到现有的模型中。

- [ ] [2-10-1  梯度提升和梯度下降有什么区别和联系？](#2-10-1)

两者都是在每一轮迭代中，利用损失函数相对于模型的负梯度方向的信息来对当前模型进行更新，只不过在梯度下降中，模型是以参数化的形式表示，从而模型的更新等价于参数的更新。

而在梯度提升中，模型并不需要参数化表示，而是直接定义在函数空间中，从而大大扩展了可以使用的模型种类。



- [ ] [2-10-2  你是如何理解Boosting和Bagging？他们有什么异同？](#2-10-2)

Bagging通过模型集成降低方差，提高弱分类器的性能。

Boosting通过模型集成降低偏差，提高弱分类器性能。



|      | Bagging                | Boosting             |
| ---- | ---------------------- | -------------------- |
| 降低 | 方差                   | 偏差                 |
| 训练 | 各个弱分类器可独立训练 | 弱分类器需要依次生成 |
|      |                        |                      |
|      |                        |                      |



- [ ] [2-10-3  讲解GBDT的训练过程？](#2-10-3)

用每个样本的残差训练下一棵树，直到残差收敛到某个阈值以下，或者树的总数达到某个上限为止。

- [ ] [2-10-4  你觉得GBDT训练过程中哪些环节可以平行提升训练效率？](#2-10-4)

决策树内部局部并行。

- [ ] [2-10-5  GBDT的优点和局限性有哪些？](#2-10-5)

**优点**

（1）预测阶段计算速度块，树与树之间可并行化计算

（2）在分布稠密的数据集上，泛化能力和表达能力都很好

（3）采用决策树作为弱分类器使得GBDT模型具有较好的可解释性和鲁棒性，能够自动发现特征间的高阶关系，并且不需要对数据进行特殊的预处理如归一化等

**缺点**

（1）GBDT在高维稀疏的数据集上，表现不如支持向量机或者神经网络

（2）GBDT在处理文本分类特征问题上，优势不如在处理数值特征时明显

（3）训练过程需要串行训练，只能在决策树内容采用一些局部并行手段提高训练速度



- [ ] [2-10-6  GBDT是否对异常值敏感，为什么？](#2-10-6)
- [ ] [2-10-7  如何防止GBDT过拟合？](#2-10-7)
- [ ] [2-10-8  在训练过程中哪些参数对模型效果影响比较大？这些参数造成影响是什么？](#2-10-8)



#### **k-means**

- [ ] [2-11-1  简述kmeans建模过程？](#2-11-1)
- [ ] [2-11-2  Kmeans损失函数是如何定义？](#2-11-2)
- [ ] [2-11-3  你是如何选择初始类族的中心点？](#2-11-3)
- [ ] [2-11-4  如何提升kmeans效率？](#2-11-4)
- [ ] [2-11-5  常用的距离衡量方法有哪些？他们都适用什么类型问题？](#2-11-5)
- [ ] [2-11-6  Kmeans对异常值是否敏感？为什么？](#2-11-6)
- [ ] [2-11-7  如何评估聚类效果？](#2-11-7)
- [ ] [2-11-8  超参数类的个数k如何选取？](#2-11-8)
- [ ] [2-11-9  Kmeans有哪些优缺点？是否有了解过改进的模型，举例说明？](#2-11-9)
- [ ] [2-11-10  试试证明kmeans算法的收敛性](#2-11-10)
- [ ] [2-11-11  除了kmeans聚类算法之外，你还了解哪些聚类算法？简要说明原理](#2-11-11)



#### **PCA降维**

- [ ] [2-12-1  为什么要对数据进行降维？它能解决什么问题？](#2-12-1)
- [ ] [2-12-2  你是如何理解维度灾难？](#2-12-1)
- [ ] [2-12-3  PCA主成分分析思想是什么？](#2-12-1)
- [ ] [2-12-4  如何定义主成分？](#2-12-1)
- [ ] [2-12-5  如何设计目标函数使得降维达到提取主成分的目的？](#2-12-1)
- [ ] [2-12-6  PCA有哪些局限性？如何优化](#2-12-1)
- [ ] [2-12-7  线性判别分析和主成分分析在原理上有何异同？在目标函数上有何区别和联系？](#2-12-1)



## **3、 深度学习**

#### **DNN**

- [ ] [3-1-1  描述一下神经网络？推倒反向传播公式？](#3-1-1)
- [ ] [3-1-2  讲解一下dropout原理？](#3-1-2)
- [ ] [3-1-3  梯度消失和梯度膨胀的原因是什么？有什么方法可以缓解？](#3-1-3)
- [ ] [3-1-4  什么时候该用浅层神经网络，什么时候该选择深层网络](#3-1-4)
- [ ] [3-1-5  Sigmoid、Relu、Tanh激活函数都有哪些优缺点？](#3-1-5)

**Sigmoid**
$$
f(x) = \frac{1}{1+ exp(-x)} \\
f'(x) = f(x)(1-f(x))
$$
优点：

缺点：（1）需要计算指数，速度慢（2）会产生梯度消失问题





**Tanh**
$$
f(x) = tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \\

f'(x) = 1 - (f(x))^2
$$
优点：

缺点：（1）需要计算指数，速度慢（2）会产生梯度消失问题



**Relu**
$$
f(x) = max(x,0) \\
\begin{equation}
f'(x)=\left\{
\begin{aligned}
    1,x>0 \\
    0,x\leq0 \\
\end{aligned}
\right.
\end{equation}
$$
优点：（1）从计算的角度上，sigmoid和tanh都需要计算指数，复杂度高，而ReLU只需要一个阈值就可以得到激活值

（2）ReLU的非饱和性可以有效解决梯度消失的问题，提供相对宽的激活边界

（3）ReLU的单侧抑制提供了网络的稀疏表达能力（防止过拟合）

缺点：（1）训练过程中会导致神经元死亡的问题

缺点：（1）训练过程中会导致神经元死亡的问题



**Leaky ReLU**
$$
\begin{equation}
f(x)=\left\{
\begin{aligned}
    x,x>0 \\
    ax,x\leq0 \\
\end{aligned}
\right.
\end{equation}
\\

f'(x)=\left\{
\begin{aligned}
    1,x>0 \\
    a,x\leq0 \\
\end{aligned}
\right.
$$
优点：实现单侧抑制，又保留了部分附体度信息以致不完全消失

缺点：a值需要人工选择





- [ ] [3-1-6  写出常用激活函数的导数](#3-1-6)

见上

- [ ] [3-1-7  训练模型的时候，是否可以把网络参数全部初始化为0？为什么](#3-1-7)

不可以；参数全部为0时，网络不同神经元的输出必然相同，相同输出则导致梯度更新完全一样，会使得更新后的参数仍然保持完全相同。从而使得模型无法训练。



- [ ] [3-1-8  Batchsize大小会如何影响收敛速度？](#3-1-8)



#### **CNN**

- [ ] [3-2-1  简述CNN的工作原理？](#3-2-1)

CNN利用了图像的三个性质：

（1）图像的pattern通常比整张图像小

（2）通用的patterns会出现在图像的不同区域

（3）对图像进行子采样并不影响图像的识别

CNN通过卷积层+pooling层不断堆积，从小的pattern开始不断识别到大的pattern，从而识别整张图像。

> CNN适合处理什么问题

具有以上三个特性的问题

- [ ] [3-2-2  卷积核是什么？选择大卷积核和小卷积核有什么影响？](#3-2-2)
- [ ] [3-2-3  你在实际应用中如何设计卷积核？](#3-2-3)
- [ ] [3-2-4  为什么CNN具有平移不变性？](#3-2-4)

参数共享



- [ ] [3-2-5  Pooling操作是什么？有几种？作用是什么？](#3-2-5)
- [ ] [3-2-6  为什么CNN需要pooling操作？](#3-2-6)
- [ ] [3-2-7  什么是batchnormalization？它的原理是什么？在CNN中如何使用？](#3-2-7)
- [ ] [3-2-8  卷积操作的本质特性包括稀疏交互和参数共享，具体解释这两种特性以其作用？](#3-2-8)

**稀疏交互**：每个神经元的只跟上一层的某些神经元连接（vs DNN全连接），用到较少参数

**参数共享**：同一层的不同神经元之间共享部分权重，用到比原来更少的参数



- [ ] [3-2-9  你是如何理解fine-tune？有什么技巧](#3-2-9)

- [ ] [3-2-10  怎么观察CNN每个神经元学到了什么](#3-2-10)

假设第k个filter是一个11 x 11 的矩阵（一个神经元），可以用以下系数来表示第k个filter被激活的程度
$$
a^k = \sum_{i=1}^{11} \sum_{i=1}^{11} a_{ij}^k
$$
并通过梯度上升找到使 $a^k$ 最大的x，该x表示的图像表示该filter对应的检测纹路。
$$
x^* = \mathop{arg \ \rm max}\limits_{x}  \ a^k
$$


#### **RNN**

- [ ] [3-3-1  简述RNN模型原理，说说RNN适合解决什么类型问题？为什么](#3-3-1)

- [ ] [3-3-2  RNN和DNN有何异同？](#3-3-2)

- [ ] [3-3-3  RNN为什么有记忆功能？](#3-3-3)

- [ ] [3-3-4  长短期记忆网络LSTM是如何实现长短期记忆功能的？](#3-3-4)

- [ ] [3-3-5  长短期记忆网络LSTM各模块都使用什么激活函数，可以使用其他激活函数么？](#3-3-5)

- [ ] [3-3-6  GRU和LSTM有何异同](#3-3-6)

- [ ] [3-3-7  什么是Seq2Seq模型？该模型能解决什么类型问题？](#3-3-7)

- [ ] [3-3-8  注意力机制是什么？Seq2Seq模型引入注意力机制主要解决什么问题？](#3-3-8)




## 4、 基础工具

#### **Spark**

- [ ] [4-1-1  什么是宽依赖，什么是窄依赖？哪些算子是宽依赖，哪些是窄依赖？](#4-1-1)
- [ ] [4-1-2  Transformation和action算子有什么区别？举例说明](#4-1-2)
- [ ] [4-1-3  讲解sparkshuffle原理和特性？shuffle write 和 huffleread过程做些什么？](#4-1-1)
- [ ] [4-1-4  哪些spark算子会有shuffle？](#4-1-1)
- [ ] [4-1-5  讲解sparkschedule（任务调度）？](#4-1-1)
- [ ] [4-1-6  Sparkstage是如何划分的？](#4-1-1)
- [ ] [4-1-7  Sparkcache一定能提升计算性能么？说明原因？](#4-1-1)
- [ ] [4-1-8  Cache和persist有什么区别和联系？](#4-1-1)
- [ ] [4-1-9  RDD是弹性数据集，“弹性”体现在哪里呢？你觉得RDD有哪些缺陷？](#4-1-1)
- [ ] [4-1-10  当GC时间占比很大可能的原因有哪些？对应的优化方法是？](#4-1-1)
- [ ] [4-1-11  park中repartition和coalesce异同？coalesce什么时候效果更高，为什么](#4-1-1)
- [ ] [4-1-12  Groupbykey和reducebykey哪个性能更高，为什么？](#4-1-1)
- [ ] [4-1-13  你是如何理解caseclass的？](#4-1-1)
- [ ] [4-1-14  Scala里trait有什么功能，与class有何异同？什么时候用trait什么时候用class](#4-1-1)
- [ ] [4-1-15  Scala 语法中to 和 until有啥区别](#4-1-1)
- [ ] [4-1-16  讲解Scala伴生对象和伴生类](#4-1-1)





#### **Xgboost**

- [ ] [4-2-1  你选择使用xgboost的原因是什么？](#4-2-1)
- [ ] [4-2-2  Xgboost和GBDT有什么异同？](#4-2-2)
- [ ] [4-2-3  为什么xgboost训练会那么快，主要优化点事什么？](#4-2-3)
- [ ] [4-2-4  Xgboost是如何处理缺失值的？](#4-2-4)
- [ ] [4-2-5  Xgboost和lightGBM有哪些异同？](#4-2-5)
- [ ] [4-2-6  Xgboost为什么要使用泰勒展开式，解决什么问题？](#4-2-6)
- [ ] [4-2-7  Xgboost是如何寻找最优特征的？](#4-2-7)



#### **Tensorflow**

- [ ] [4-3-1  使用tensorflow实现逻辑回归，并介绍其计算图](#4-3-1)
- [ ] [4-3-2  sparse_softmax_cross_entropy_with_logits和softmax_cross_entropy_with_logits有何异同？](#4-3-2)
- [ ] [4-3-3  使用tensorflow过程中，常见调试哪些参数？举例说明](#4-3-3)
- [ ] [4-3-4  Tensorflow梯度更新是同步还是异步，有什么好处？](#4-3-4)
- [ ] [4-3-5  讲解一下TFRecords](#4-3-5)
- [ ] [4-3-6  tensorflow如何使用如何实现超大文件训练？](#4-3-6)
- [ ] [4-3-7  如何读取或者加载图片数据？](#4-3-7)



## **5、推荐系统**

- [ ] [5-1-1  你是如何选择正负样本？如何处理样本不均衡的情况？](#5-1-1)

样本不均衡处理：

1）上采样和子采样；2）修改权重（修改损失函数）；3）集成方法：bagging，类似随机森林、自助采样；4）多任务联合学习；



- [ ] [5-1-2  如何设计推荐场景的特征体系？举例说明](#5-1-1)
- [ ] [5-1-3  你是如何建立用户模型来理解用户，获取用户兴趣的？](#5-1-1)
- [ ] [5-1-4  你是如何选择适合该场景的推荐模型？讲讲你的思考过程](#5-1-1)
- [ ] [5-1-5  你是如何理解当前流行的召回->粗排->精排的推荐架构？这种架构有什么优缺点？什么场景适用使用，什么场景不适合？](#5-1-1)
- [ ] [5-1-6  如何解决热度穿透的问题？（因为item热度非常高，导致ctr类特征主导排序，缺少个性化的情况）](#5-1-1)
- [ ] [5-1-7  用户冷启动你是如何处理的？](#5-1-1)
- [ ] [5-1-8  新内容你是如何处理的？](#5-1-1)
- [ ] [5-1-9  你们使用的召回算法有哪些？如何能保证足够的召回率？](#5-1-1)
- [ ] [5-1-10  实时数据和离线数据如何融合？工程上是怎样实现？如何避免实时数据置信度不高带来的偏差问题？](#5-1-1)
- [ ] [5-1-11  你们是如何平衡不同优化目标的问题？比如：时长、互动等](#5-1-1)
- [ ] [5-1-12  不同类型内容推荐时候，如何平衡不同类型内容，比如图文、视频；或者不同分类](#5-1-1)
- [ ] [5-1-13  如何保证线上线下数据一致性？工程上是如何实现？](#5-1-1)
- [ ] [5-1-14  离线训练效果好，但是上线效果不明显或在变差可能是什么问题？如何解决？](#5-1-1)
- [ ] [5-1-15  在实际业务中，出现badcase,你是如何快速反查问题的？举例说明](#5-1-1)
- [ ] [5-1-16  使用ctr预估的方式来做精排，会不会出现相似内容大量聚集？原因是什么？你是如何解决的？](#5-1-1)
- [ ] [5-1-17  你了解有多少种相关推荐算法？各有什么优缺点](#5-1-1)



|               | 优点                     | 缺点                       | 适用场景                                       |
| ------------- | ------------------------ | -------------------------- | ---------------------------------------------- |
| item-based CF | 注重个性化，发掘长尾商品 | 推荐结果过于热门，需要惩罚 | item更新频率低的（如购物网站、图书、电影网站） |
| user-based CF | 社交网络                 |                            | item更新频率高（新闻）                         |
|               |                          |                            |                                                |

l

- [ ] [5-1-18  深度学习可以应用到推荐问题上解决哪些问题？为什么比传统机器学习要好？](#5-1-1)



# **二、数学相关**

## **6、 概率论和统计学**

- [ ] [6-1-1  说说你是怎样理解信息熵的？](#6-1-1)
- [ ] [6-1-2   能否从数据原理熵解析信息熵可以表示随机变量的不确定性？](#6-1-2)
- [ ] [6-1-3  怎样的模型是最大熵模型？它有什么优点](#6-1-3)
- [ ] [6-1-4  什么是Beta分布？它与二项分布有什么关系？](#6-1-4)
- [ ] [6-1-5   什么是泊松分布？它与二项分布有什么关系？](#6-1-5)
- [ ] [6-1-6  什么是t分布？他与正态分布有什么关系？](#6-1-6)
- [ ] [6-1-7   什么是多项式分布？具体说明？](#6-1-7)
- [ ] [6-1-8   参数估计有哪些方法？](#6-1-8)
- [ ] [6-1-9  点估计和区间估计都是什么？](#6-1-9)
- [ ] [6-1-10  讲解一下极大似然估计，以及适用场景？](#6-1-10)

**极大似然估计中采样需满足一个重要的假设，就是所有的采样都是独立同分布的。**

那么我们就知道了极大似然估计的核心关键就是对于一些情况，样本太多，无法得出分布的参数值，可以采样小样本后，利用极大似然估计获取假设中分布的参



## 7、 最优化问题

- [ ] [7-1-1  什么是梯度？](#7-1-1)
- [ ] [7-1-2  梯度下降找到的一定是下降最快的方法？](#7-1-1)
- [ ] [7-1-3  牛顿法和梯度法有什么区别？](#7-1-1)
- [ ] [7-1-4  什么是拟牛顿法？](#7-1-1)
- [ ] [7-1-5  讲解什么是拉格朗日乘子法、对偶问题、kkt条件?](#7-1-1)
- [ ] [7-1-6  是否所有的优化问题都可以转化为对偶问题？](#7-1-1)
- [ ] [7-1-7  讲解SMO（SequentialMinimalOptimization）算法基本思想？](#7-1-1)
- [ ] [7-1-8  为什么深度学习不用二阶优化？](#7-1-1)
- [ ] [7-1-9  讲解SGD，ftrl、Adagrad，Adadelta，Adam，Adamax，Nadam优化算法以及他们的联系和优缺点](#7-1-1)
- [ ] [7-1-10 为什么batch size大，训练速度快or为什么mini-batch比SGD快？](#7-1-10)

（1）从矩阵计算角度

mini-batch不同的输入可以拼成一个矩阵，和W计算矩阵相乘可以并行地计算，比SGD一个一个计算快。对于GPU，矩阵相乘可以并行地处理，速度快。

- [ ] [7-1-11  7-1-11  为什么不把batch size设得特别大](7-1-11)

（1）GPU内存限制

（2）性能差；容易卡在局部极小值  





